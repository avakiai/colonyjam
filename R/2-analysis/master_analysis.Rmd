---
title: "Flexible vocal timing adaptation to predictable and unpredictable playback noise"
author: "Ava Kiai"
output:
  html_document: 
    toc: yes
geometry: margin=1in
fontsize: 11pt
params:
  data_path: './../1-data' 
  results_path: './../3-results'
  exp: "exp_1"
  plot_sm: TRUE
  use_all: TRUE
  subsample: FALSE
  seed: 42
---
### About
In this experiment, we played amplitude modulated white noise to a group of bats in order to investigate their ability to temporally modulate their calling behavior at the group level. 

We hypothesized that bats would adapt the timing of their calls to avoid temporal overlap with the amplitude modulated noise.

After extracting call timings from acoustic recordings (.wav files) and identifying the instantaneous period (or phase) in the real or simulated (as in the silent condition) amplitude modulation cycle, we analyzed the call onset data, primarily with circular statistics. 

### Setup

```{r knit-options, setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r set-working-directory, eval=FALSE, include=FALSE}
setwd(here::here())
getwd()
```

```{r load-packages, echo=TRUE, message=FALSE, warning=FALSE}
packages <- c("circular", "CircStats", "CircSpaceTime", "sjPlot", "janitor", "broom", "knitr",  "kableExtra", "car", "MASS", "emmeans", "rcompanion", "pscl", "caret", "MLeval", "viridis", "scales","ggh4x","ggside", "ggdist", "ggridges","tidyverse")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))
```

```{r load-functions, include=FALSE}
# Paths & Source functions
sapply(list.files('./functions', pattern = ".R", full.names = T), source)
```

### Parameters
```{r injected-parameters}
str(params)

# local paths
data_path <- params$data_path
results_path <- params$results_path
tables_path <- file.path(results_path,'tables')
  if (!dir.exists(tables_path)) dir.create(tables_path)
figures_path <- file.path(results_path,'figures')
  if (!dir.exists(figures_path)) dir.create(figures_path)
sm_path <- file.path(results_path,'supplementary_figures')
  if (!dir.exists(sm_path)) dir.create(sm_path)

# experiment number
exp <- params$exp

# seed for subsampling
seed <- params$seed

# whether to use all data for circular stats
use_all <- params$use_all
subsample <- params$subsample

# whether to plot supplementary plots/tables
supplementary <- params$plot_sm
```

```{r static-parameters, include=FALSE}
options(scipen=999,
        dplyr.summarise.inform=FALSE)

# color palettes and figure dimensions
if (exp=="exp_1") {c_pal <- c("#0F6273","#D66502","#F5C951") # playback conditions
                   pal <- c("#F46D43","#FFA054") # modulation rates
                   w <- 3
                   w2 <- 3
                   w3 <- 3
                   h <- 4
                   h2 <- 5}
if (exp=="exp_2") {c_pal <- c("#0F6273", "#D66502", "#E0685D")
                   pal <- RColorBrewer::brewer.pal(n=8, name="Spectral")
                   w <- 11
                   w2 <- 12
                   w3 <- 6
                   h <- 6
                   h2 <- 12}
```

#### Load data
```{r load-data}
# load data
data <- readr::read_rds(file.path(data_path,'input',paste0(exp, "_data.RDS"))) %>%
  mutate(start_phase_circ = circular(start_phase, 
                                     units="radians", 
                                     zero = 0, 
                                     modulo = "2pi", 
                                     rotation = "counter"),
         .after ="start_phase") 

# data for averaging histograms (modulation variable is numeric-ish)
data_2proc <- data %>%
  mutate(modulation = factor(modulation, 
                             levels = sort(unique(data$modulation))))

# data for remaining analyses (modulation variable is character-ish, better for plotting)
data <- data %>%
  mutate(modulation = factor(modulation, 
                             levels = sort(unique(data$modulation)), 
                             labels = paste0(as.character(sort(unique(data$modulation))),"Hz")))

# data from the first few minutes of exposure
data_init <- filter_firstexposure(data_2proc)

str(data)

# save interim data manips to 1-data/output
data_path <- file.path(data_path,'output')
  if (!dir.exists(data_path)) dir.create(data_path)

```


### Summary Stats

A few rough summary statistics. 
Note that each row is an observation (call event), with parameters:

- group: experimental group (1:4) (Note that for experiment 1, there were 8 groups
of bats tested in each modulation rate context, but each is labelled 1-4.)
- session: recording day (1:5)
- part: experimental block (1:3), silent condition is always part 1
- condition: playback condition
- modulation: amplitude modulation rate for masking conditions
- minute: minute since the start of the recording file in which the call occurred
- start_seconds: timestamps of call onsets in seconds since the start of the file
- stop_seconds: ibid. for call offsets
- start_sp: samples since start of file at call onset
- end_sp: ibid. for call offsets
- start_cycle: nth cycle of the corresponding modulation rate in the corresponding file at call onset
- end_cycle: ibid. at call onset
- start_phase: moment in the corresponding modulation cycle (in radians) at call onset
- start_phase_circ: ibid., but of class circular
- end_phase: ibid. at call offset
- start_per: timepoint in the corresponding modulation period (in seconds) at call onset
- end_per: ibid. at call offset
- duration: call offset - call onset (in seconds)
- onset_interval: call onset - previous call's call onset (is NA if call is the first in the file)
- class: type of call (for now, all are labelled "call")
- file_no: index of recording file, indicating the order of recorded files from the corresponding experimental block
- familiarization: number of days in which bats were housed together as a group prior to the first recording day
- order: randomization/counter-balancing order for the presentation of masking conditions

```{r summary-alldata}
#summary(data)
```

Total number of calls:
```{r n}
nrow(data)
#1,425,067 # exp 1
#1,088,994 # exp 2
# = 2,514,061 # total
```

Call durations:
```{r durations}
summary(data$duration)

IQR(data$duration)
```


### Distribution of call onsets in the modulation cycle

Calculate histogram averages (essentially, a PSTH)

```{r average-histograms}

call_summary1 <- get_avg_hist(data_2proc, NULL, list("group","session"), "period", 
                              by_group = TRUE, by_session = TRUE)
call_summary60 <- get_avg_hist(data_2proc, 60, list("group","session"), "period")

# optionally: check that the two have the correct # of bins
# call_summary1$generic %>% group_by(modulation) %>% summarise(sort(unique(mids),decreasing = T)[2])
# call_summary60$generic %>% group_by(modulation) %>% summarise(n=length(unique(mids)))

```

First, we asked: Did the presence of playback noise affect the distribution of emitted calls within the modulation cycle?

Here we plot an averaged histogram of detected call onsets 
(binned calls (1 ms bins) summed over all cycles, averaged over groups and recording days):

Points are mean # calls, shaded areas are +- sem...

```{r p-averaged-histograms, echo=TRUE, out.width = "50%"}

# average call counts all at once, 1 ms bins
p <- p.hist_avg_dot_kwargs(dat = call_summary1$generic, facets = c("condition","modulation"))
 print(p)
 ggsave(file.path(figures_path,paste0(exp, '_fig2a.png') ), p, width = w, height = 5)

# average call counts with fixed y-axis, to better illustrate the relative calling rate between conditions
p2 <- p.hist_avg_dot_kwargs_fixedy(dat = call_summary1$generic, facets = c("","modulation")) 
  print(p2)
 ggsave(file.path(figures_path,paste0(exp, '_fig2a-alt1.png') ), p2, width = w, height = 5)

# average call counts with fixed y-axis, with 60 ms bins for each modulation rate, to show trend on the same x-scale across conditions  
p3 <- p.hist_avg_dot_kwargs_fixedy(dat = call_summary60$generic, facets = c("","modulation")) 
  print(p3)
  ggsave(file.path(figures_path,paste0(exp, '_fig2a-alt2.png') ), p3, width = w, height = 5) 
```

```{r sp-averaged-histograms, echo=supplementary, eval=supplementary, out.width = "30%"}
# show average call counts (with and without fixed y axis) for each group, and for each recording day ("session")
for (m in sort(unique(call_summary1$group$modulation))) {
 p1 <- p.hist_avg_dot_kwargs(dat = call_summary1$group %>% filter(modulation == m), 
                             facets = c("condition","group")) +
                             ggtitle(paste0(m, ": by group"))
 p2 <- p.hist_avg_dot_kwargs_fixedy(dat = call_summary1$group %>% filter(modulation == m), 
                                    facets = c("","group")) +
                             ggtitle(paste0(m, ": by group")) 
 p3 <- p.hist_avg_dot_kwargs(dat = call_summary1$session %>% filter(modulation == m),
                             facets = c("condition","session")) +
                             ggtitle(paste0(m, ": by day"))
 
 print(p1); print(p2); print(p3)
 ggsave(file.path(sm_path,paste0(exp, '_figS2_',m,'.png') ), 
        p1, width = 7, height = 5)
 ggsave(file.path(sm_path,paste0(exp, '_figS2_',m,'-alt1.png') ), 
        p2, width = 7, height = 5)
 ggsave(file.path(sm_path,paste0(exp, '_figS2_',m,'.png') ), 
        p3, width = 7, height = 5)
}
```

```{r sp-averaged-histograms-initial, echo=supplementary, eval=supplementary, out.width = "50%"}
# show average hist of binned calls for the first 7.5-8 mins of continuous AM playback for each mod.
# [not included in paper, but illustrative]
call_summary1_init <- get_avg_hist(data_init, NULL, list("group"), "period", by_group = TRUE)
#str(data_init)
 
firsts <- p.hist_avg_col(dat = call_summary1_init$group %>% 
                 dplyr::filter(condition=="full-band masker" | condition=="steady-state masker"), 
               facets = c("modulation","group"))

print(firsts)

ggsave(file.path(sm_path,paste0(exp, '_figSx.png') ), firsts, width = 7, height = h2)

```

### Circular Statistics
To analyze the distribution of call onsets within the modulation cycle, we must
use circular statistics (with functions from the package `circular`), since the 
nature of our data is cyclical.

First we will compute the following summary statistics:

- Mean Resultant Direction ($\bar{\theta}$)
- Mean Resultant Length ($\bar{R}$)
- Circular Variance ($Vm$): $1-R$
- Circular Standard Deviation ($v$): $\sqrt{1-2*log(\bar{R})}$                             
- MLE Mu ($\mu$): mean parameter estimated from the maximum likelihood
von Mises distribution (`circular::mle.vonmises()`)
- MLE Kappa ($\kappa$): concentration parameter estimated from the 
maximum likelihood von Mises distribution (`circular::mle.vonmises()`)
- MLE CIs for $\bar{\theta}$: confidence intervals for the mean
parameters estimated from the maximum likelihood von Mises distribution 
(`circular::mle.vonmises.bootstrap.ci()`)

Then, we'll get bootsrapped mean and concentration parameters from the whole dataset.

-----

We predicted that calls emitted in presence of modulated noise would tend to
"cluster" unimodally in the hemisphere corresponding to the amplitude trough. 
Conversely, we predicted that the distribution of call onsets emitted in silence
would show no clustering, and therefore resemble a uniform distribution.

This prediction entails two specific hypotheses:

1. Distribution of calls detected in silence will be equivalent to a uniform distribution, 
while that of calls detected in the presence of masking noise will deviate from the uniform distribution.
  --> Method: Rayleigh test of uniformity (`circular::rayleigh.test(x, mu=circular(0))`)

2. The distribution of calls in the playback conditions will each besignificantly different from the silence condition.
  --> Method: Mardia-Watson-Wheeler non-parametric test of homogeneous samples (`circular::watson.wheeler.test(x,y)`),
      where differences between samples can be in either the mean or the variance.

#### Calculate summary and distribution statistics

```{r subsample-data, include=FALSE}

if (subsample) {
  check_n <- data %>% group_by(condition, modulation) %>% summarise(n = n())
  
  set.seed(seed) #.Random.seed)
  data_samp <- data %>% group_by(condition, modulation) %>% slice_sample(n = min(check_n$n))
  
  print(paste0("Sub-sampled data: N = ", nrow(data_samp), ". Original data: N = ", nrow(data), ". N in each group = ", min(check_n$n)))
  
  # if circ data exists, prepare for loading
  f_circsum <- file.path(data_path,paste0("prc_", exp, "_circ_data-seed",seed,".RDS"))

} else {
  data_samp <- data
  
  f_circsum <- file.path(data_path,paste0("prc_", exp, "_circ_dat-all.RDS"))
}
```

**In which conditions were call onset distributions unimodally clustered?**

```{r calculate-circular-stats, message=FALSE, warning=FALSE}
# prepare data for analysis: split into lists, one for each modulation x condition case
# data is subsampled if requested in header

data_split <-  data_samp %>% group_by(modulation, condition) %>% group_split()

# label and convert to circular data
circ_data <- lapply(data_split, function(x) 
  list(modulation = unique(x$modulation),
       condition = unique(x$condition),
       phase = x$start_phase_circ)) 


if (file.exists(f_circsum)) { # as RDS...
  circ_summary <- readr::read_rds(f_circsum)
  
} else {
  # calculate values:
  circ_datsum <- lapply(circ_data, 
         function(x) tibble(modulation = x$modulation, 
                            condition = x$condition,
                            n = length(x$phase),
                            theta_bar = mean.circular(x$phase, na.rm = T), # angular mean
                            r_bar = rho.circular(x$phase, na.rm = T), # mean resultant length
                            vm = var.circular(x$phase), 
                            v = sd.circular(x$phase),  
                            # Test of uniformity:
                            rt = rayleigh.test(x$phase, mu=circular(0))$statistic,
                            p = rayleigh.test(x$phase, mu=circular(0))$p.value,
                            # MLE von Mises:
                            mle_mu = mle.vonmises(x$phase)$mu,
                            bs_mu_ci_l = mle.vonmises.bootstrap.ci(x$phase, alpha=.05)$mu.ci[1], 
                            bs_mu_ci_h = mle.vonmises.bootstrap.ci(x$phase, alpha=.05)$mu.ci[2],
                            mle_kappa = mle.vonmises(x$phase)$kappa) 
                        )  
  
  # wrangle & correct p values
  circ_summary <- tibble(do.call(rbind,circ_datsum))  %>%
                  mutate(p_adjust = p.adjust(p, method ="bonferroni"), .after = "p") %>%
                  mutate(across(where(is.numeric), round, 3))
  
  head(circ_summary)
  
  # save, if first time
  if (subsample) {
    readr::write_rds(circ_summary, file = file.path(data_path,paste0("prc_", exp, "_circ_dat-seed",seed,".RDS")))
  } else {
    readr::write_rds(circ_summary, file = file.path(data_path,paste0("prc_", exp, "_circ_dat-all.RDS")))
  }

}

circ_summary
```

```{r t-circ-stats, include=FALSE}
circ_summary %>% 
  kbl(escape = FALSE, 
      booktabs = TRUE, 
      format = "html",
      caption = "Circular Statistics") %>%
  kable_styling() %>%
  add_header_above(c(" " = 2, "Summary Statistics" = 5,
                     "Rayleigh's Test" = 3, "MLE von Mises" = 4)) %>%
  kable_styling(font_size = 8, full_width = TRUE) %>%
  column_spec(1, width = "20mm") %>%
  column_spec(2, width = "60mm") %>%
  column_spec(2:13, width = "20mm") %>%
  kable_paper() %>%
  cat(., file = file.path(tables_path,paste0(exp, "_circ_stats.html")))
```


#### Plot circular call density
Now, let's plot our call count data (as a density rather than raw call count) as before,
but on the polar plane and indicate the mean direction and resultant length of the resultant
vector.

```{r fix-confint-range, include=FALSE}
# if confidence intervals for angular means span 0, split the span over two rows for accurate plotting
circ_summary_plot <- confint_wrap(circ_summary) 
```

```{r p-circular-density-histograms, out.width = "80%"}
circ_density <- p.circ_density(dat = data_samp, circ_dat = circ_summary_plot, bins = 30) %>% recolor(which="f")

circ_density

if (subsample) {
  ggsave(file.path(figures_path,paste0(exp, "_fig2b_",seed,".png")), circ_density, width = w, height = 5)
} else {
  ggsave(file.path(figures_path,paste0(exp, '_fig2b.png')), circ_density, width = w, height = 5)
}
```


#### Calculate bootstrapped angular vectors 

**How consistent was the clustering, if present?**

```{r use-all-eval-boot, include=FALSE}

if (subsample & use_all) {
  wdata_split <-  data %>% group_by(modulation, condition) %>% group_split()
  wcirc_data <- lapply(wdata_split, function(x)
   list(modulation = x$modulation[1],
        condition = x$condition[1],
        phase = x$start_phase_circ))
} else{
  wcirc_data <- circ_data
}
```

```{r calculate-boot-parameters, include=FALSE}
if (use_all) {
  f_mu_bs <- file.path(data_path,paste0("prc_", exp, "_boot_mu_kappa-all.RDS"))
} else {
  f_mu_bs <- file.path(data_path,paste0("prc_", exp, "_boot_mu_kappa-seed",seed,".RDS"))
} 

if (file.exists(f_mu_bs)) {
  mu_bs_df <- readr::read_rds(f_mu_bs)

} else {
  # calculate bootstrapped means & concentrations: 1000 per condition x modulation
  mu_bs <- lapply(wcirc_data, function(x) list(modulation = x$modulation, 
                                            condition = x$condition, 
                                            mu = as.numeric(mle.vonmises.bootstrap.ci(x$phase)$mu),
                                            kappa = as.numeric(mle.vonmises.bootstrap.ci(x$phase)$kappa),
                                            ci_lo = as.numeric(mle.vonmises.bootstrap.ci(x$phase)$mu.ci[1]),
                                            ci_hi = as.numeric(mle.vonmises.bootstrap.ci(x$phase)$mu.ci[2]))) 
  mu_bs_df <- do.call(rbind, lapply(mu_bs, function(x) as.data.frame(x)))
  
  if (use_all) {
    save_name = file.path(data_path,paste0("prc_", exp, "_boot_mu_kappa-all"))
    mu_bs_df.samp <- mu_bs_df
  } else {
    save_name = file.path(data_path,paste0("prc_", exp, "_boot_mu_kappa-seed",seed))
  }
  readr::write_rds(mu_bs_df, file = paste0(save_name, ".RDS"))
}

head(mu_bs_df)
```


#### Plot bs parameters in the polar plane
```{r p-boot-circ, out.width = "50%"}
circ_params <- p.circ_params(mu_bs_df, compress = T)  %>% recolor()


circ_params

ggsave(file.path(figures_path,paste0(exp, '_fig2c.png')), circ_params, width = 3, height = 6)
```

##### ... and in the cartesian plane
```{r p-boot-sides, out.width = "50%"}
mu_kappa <- p.mu_kappa(dat = mu_bs_df %>% 
                         group_by(modulation, condition) %>% 
                         slice_sample(n = 500)) %>% recolor()
mu_kappa

ggsave(file.path(figures_path,paste0(exp, '_figS3-500.png') ), mu_kappa, width = 6, height = 6)

```


#### [Mardia]-Watson-Wheeler Homogeneity of Samples

We now compute frequentist non-parametric analyses for whether the distributions of
calls observed in our different modulation and masking conditions are identical.

The null hypothesis is that the distributions are identical, either in their means
or their variances:

##### Compare playback conditions within modulation rate contexts

**Did distributions of call onsets differ between playback conditions at each modulation rate?**

```{r watson-wheeler-test}

circ_data_df <- do.call(rbind, lapply(circ_data, function(x) as.data.frame(x)))

ww_tests <- lapply(sort(unique(circ_data_df$modulation)), function(x) 
  watson.wheeler.test(phase ~ condition,circ_data_df[circ_data_df$modulation==x,]) %>%
    broom::tidy() %>%
     mutate(across(where(is.numeric), round, 3)) %>%
      mutate(modulation = x,.before=1)) %>%
  do.call(rbind,.) %>%
  mutate(p.adjust = p.adjust(p.value, method ="bonferroni"), .after = "p.value") %>%
  janitor::clean_names()

ww_tests
```

```{r save-watson-wheeler, include=FALSE}
 ww_tests %>%
  kbl(escape = FALSE, 
      booktabs = TRUE, 
      format = "html",
      caption = "Watson-Wheeler test for homogeneity of groups (within modulation rates)") %>%
  kable_styling() %>%
  cat(., file = file.path(tables_path,paste0(exp, "_watson-wheeler.html")))
```


#### Rao's test of homogeneity of means, dispersions

Test if means or vector lengths were significantly different between conditions and,
separately, across modulation rates:


#### Compare playback conditions within each modulation rate

**How did distributions of call onsets differ between playback conditions at each modulation rate?**
**In their angular means or their polar concentrations?**

```{r rao-tests}
rao_testsm <- do.call(rbind, lapply(sort(unique(circ_data_df$modulation)), function(x) {
    df <- circ_data_df[circ_data_df$modulation==x,] %>% group_split(condition)
    rao <- circular::rao.test(lapply(df,'[[', 'phase'))
    data.frame(modulation = as.factor(x),
               test = as.factor(c("polar vectors", "dispersions")), 
               statistic = rao$statistic, df = rao$df, p_value = rao$p.value) %>% 
      mutate(across(where(is.numeric), round, 3)) %>% 
      mutate(p_adjust = p.adjust(p_value, method ="bonferroni"), .after = "p_value") 
    }))

# test differences for playback conditions within mod rates
rao_testsm

rao_testsc <- do.call(rbind, lapply(sort(unique(circ_data_df$condition)), function(x) {
    df <- circ_data_df[circ_data_df$condition==x,] %>% group_split(modulation)
    rao <- circular::rao.test(lapply(df,'[[', 'phase'))
    data.frame(condition = as.factor(x),
               test = as.factor(c("polar vectors", "dispersions")), 
               statistic = rao$statistic, df = rao$df, p_value = rao$p.value) %>% 
      mutate(across(where(is.numeric), round, 3)) %>% 
      mutate(p_adjust = p.adjust(p_value, method ="bonferroni"), .after = "p_value")
    }))

# test differences for mod rates within conditions
rao_testsc 

```

```{r save-rao, include=FALSE}
rao_testsm %>%
  kbl(escape = FALSE, 
      booktabs = TRUE, 
      format = "html",
      caption = "Rao test for homogeneity of angular means & dispersions") %>%
  kable_styling() %>%
  cat(., file = file.path(tables_path,paste0(exp, "_rao-modulations.html")))

rao_testsc %>%
  kbl(escape = FALSE, 
      booktabs = TRUE, 
      format = "html",
      caption = "Rao test for homogeneity of angular means & dispersions") %>%
  kable_styling() %>%
  cat(., file = file.path(tables_path,paste0(exp, "_rao-conditions.html")))
```


#### Post-hoc
Next, let's compute post-hoc comparisons for each significant test (by modulation rate)
above by comparing each pair of masking conditions within each modulation condition for both
rates, again using the Rao test.

```{r rao-post-hoc}
do_contrasts <- rao_testsm$modulation[rao_testsm$p_value<0.05]

circ_contrasts <- lapply(do_contrasts, function(x) {
  df <- circ_data_df[circ_data_df$modulation==x,]
  pairs <- t(combn(unique(df$condition), m=2))

  tests <- lapply(seq.int(nrow(pairs)), function(y) {
    condlist <- df[df$condition %in% pairs[y,],] %>% group_split(condition)
    rao <- circular::rao.test(condlist[[1]]$phase, condlist[[2]]$phase)
    data.frame(modulation = as.factor(x),
               condition = as.factor(paste(pairs[y,],collapse = " vs. ")),
               test = as.factor(c("polar vectors", "dispersions")), 
               statistic = rao$statistic, df = rao$df, p_value = rao$p.value) %>% 
      mutate(across(is.numeric, round, 3)) %>% 
      mutate(p_adjust = p.adjust(p_value, method ="bonferroni"), .after = "p_value")
    }) %>% do.call(rbind,.) 
  }
 )  %>% do.call(rbind,.)

circ_contrasts
```

```{r save-rao-post-hoc, include=FALSE}
circ_contrasts %>%
  kbl(escape = FALSE, 
      booktabs = TRUE, 
      format = "html",
      caption = "Rao test for homogeneity of angular means & dispersions within mod rates") %>%
  kable_styling() %>%
  cat(., file = file.path(tables_path,paste0(exp, "_rao-modulations_contrasts_within.html")))
```




### Rate of vocalizations

Let's look at the overall number of calls observed in the three experimental conditions
and within the two tested modulation rates.

### Negative Binomial Regression 

```{r prepare-nb-reg, message=TRUE, warning=FALSE, include=FALSE}
n_calls_reg <- data %>% group_by(modulation, condition, group, as.factor(session)) %>% summarise(n = n())
n_calls_reg
```

First we modelled our counts using a Poisson GLM. However, we observed that the 
data were highly overdispersed. For example, for the 8 Hz context, overdispersion was:

```{r check-poisson}
# Poisson Regression 
calls.glm <- glm(n ~  condition, family = poisson, data = n_calls_reg[n_calls_reg$modulation=="8Hz",])
# model summary
  # summary(calls.glm)

# check for overdispersion
OD.pr <- sum(residuals(calls.glm, type="pearson")^2)/df.residual(calls.glm)
print(OD.pr)
```

Therefore, we proceeded with a Negative Binomial GLM, which allows for overdispersion in the count variable.

We ran a model for each modulation rate separately, so that the 
reference group (Intercept) is meaningful for all model terms for our purposes, i.e.
coefficient indicate  changes in calling rate between silent baseline and each masking condition.

Our  models are given by:
$$ln(\widehat{calls_{i}}) = Intercept +
\beta_1 Mask(condition_j = 2) + \beta_3 Mask(condition_j = 3)$$

Where $i$ gives the modulation rate and $j$ gives the level of the condition manipulation. 

Thus,
$$\widehat{calls_i} = e^{Intercept + \beta_1 Mask_{condition_j = 2}+...}$$
defines the rate of calling per hour (the sampling time for each condition), given
an arbitrary combination of modulation rate and masking condition.

We also ran a "big" model that included all interactions, for which coefficients would
have a more complex meaning but analyses of variance would provide a global picture. 

#### Run all models
```{r run-nb-model-big}
big.model <-  run_negbinom_model(n_calls_reg, 
                                 formula = "n ~ condition * modulation", 
                                 contformula = "~ modulation | condition ")

# overdispersion
print(big.model$dispersion)
```


```{r run-nb-model-ea}
all.models <- lapply(sort(unique(n_calls_reg$modulation)), function(x) 
                      md <- run_negbinom_model(n_calls_reg %>% filter(modulation==x)) )
  
names(all.models) <- sort(unique(n_calls_reg$modulation))

# overdispersion
print(do.call(rbind, lapply(all.models, '[[', 'dispersion')) )
```

```{r save-model-tables, include=FALSE}
lapply(seq_along(all.models), function(x) 
      sjPlot::tab_model(all.models[[x]]$model, show.aic = TRUE, 
                        show.dev = T, show.se = T,
                        title = paste("Observed calls per hour in",names(all.models[x]),"modulated noise"), 
                        file = file.path(tables_path,paste0(exp, "_nb_",names(all.models[x]),".html")) )) 
```

#### Anova
Note that these are 1-way ANOVAs from different models put together in one table, not
a single ANOVA.
```{r nb-anova}
# pull out anova tables
(anova <- do.call(rbind, lapply(all.models, '[[', 'deviance')) %>% 
   mutate(across(is.numeric, round, 2)))
```

```{r save-model-deviance-tables, message=FALSE, warning=FALSE, include=FALSE}
 anova %>%
  kbl(escape = FALSE, 
      booktabs = TRUE, 
      format = "html",
      caption = "Type II Analysis of Deviance") %>%
  kable_styling() %>%
  cat(., file = file.path(tables_path,paste0(exp, "_deviance_tables_anova.html")))

```

Below, the analysis of variance for the large model with all features:
```{r nb-anova-big}
big.model$deviance
```

#### Incidence Rate Ratios (IRR)
The IRRs give us the change in the per hour rate [of calling] from baseline to each condition.
```{r combined-IRRs}
# IRR
IRRs <- do.call(rbind, lapply(all.models, '[[', 'IRR')) %>%
   mutate(across(is.numeric, round, 2)) %>%
   mutate(modulation = str_split(row.names(.),"[.]",simplify = T)[,1], .before=1) %>%
   mutate(coef = str_remove(str_split(row.names(.),"[.]",simplify = T)[,2],"condition"), .after=1) %>%
   remove_rownames()
IRRs
```

```{r t-IRRs, include=FALSE}
IRRkbl <- IRRs %>%
    kbl(escape = FALSE, 
      booktabs = TRUE, 
      format = "html",
      caption = "IRR") %>%
  kable_styling() 

rows <- seq(1,nrow(IRRs),length(unique(IRRs$coef)))
for (p in seq_along(unique(IRRs$modulation))) {
  pack_rows(IRRkbl, unique(IRRs$modulation)[p], rows[p], rows[p]+2)
  }
            
IRRkbl %>%
  cat(., file = file.path(results_path,paste0("/tables/",exp,"_IRRs.html"))) 


```

```{r p-model-estimates, include=FALSE}
lapply(seq_along(all.models), function(x) {
       p <- all.models[[x]]$plot 
       ggsave(file.path(sm_path,paste0(exp, '_IRR_', names(all.models[x]),'.png') ), p, width = 5, height = 3)})

#big.model$plot
```


#### Marginal mean contrasts
Below are the post-hoc contrasts from estimated marginal means:
```{r t-wrangle-post-hoc-tables}
(contrasts <- do.call(rbind, lapply(all.models, '[[', 'contrasts')) %>%
   mutate(across(is.numeric, round, 2)) %>%
   mutate(modulation = str_split(row.names(.),"[.]",simplify = T)[,1], .before=1) %>%
   remove_rownames() %>%
   dplyr::select(-c("df", "null")) %>% 
   dplyr::rename(IRR = ratio))
```


```{r t-big-model-post-hoc-table, eval=FALSE, include=FALSE}
# Contrasts for the big model:
big.model$contrasts %>%
   dplyr::select(-c("df", "null")) %>% 
   dplyr::rename(IRR = ratio)
```


```{r t-save-post-hoc-tables, include=FALSE}
# marginal means
contrasts %>%
    kbl(escape = FALSE, 
      booktabs = TRUE, 
      format = "html",
      caption = "Estimated Marginal Means") %>%
  kable_styling() %>%
  cat(., file = file.path(tables_path,paste0(exp, "_mm_contrasts.html"))) 
```

#### Model predictions
Now let's plot the model predictions to ensure it's a good fit:
```{r p-model-predictions, echo=TRUE}

preds <- do.call(rbind, lapply(all.models, '[[', 'predictions')) %>%
   mutate(across(is.numeric, round, 2)) %>%
   mutate(modulation = factor(str_split(row.names(.),"[.]",simplify = T)[,1], 
          levels = sort(unique(data$modulation))),.before=1) %>%
   remove_rownames()

n_pred <- p.n_pred(preds) %>% recolor()

n_pred
ggsave(file.path(figures_path,paste0(exp, '_fig2d.png') ), n_pred, width = 5, height = 4)

```

### Changes in call rate between modulation rate contexts

Now let's see if there were changes in the overall rate of calling between different
modulation rate contexts for the same playback condition.
```{r model-across-conditions}

all.models2 <- lapply(sort(unique(n_calls_reg$condition)), function(x) 
                      md <- run_negbinom_model(n_calls_reg %>% filter(condition==x), 
                                               formula = "n ~  modulation", 
                                               contformula = "~ modulation")  )
  
names(all.models2) <- sort(unique(n_calls_reg$condition))
```

```{r save-model-tables2, include=FALSE}
lapply(seq_along(all.models2), function(x) 
      sjPlot::tab_model(all.models2[[x]]$model, show.aic = TRUE, 
                        show.dev = T, show.se = T,
                        title = paste("Observed calls per hour in",names(all.models2[x])," noise"), 
                        file = file.path(tables_path,paste0(exp, "_nb_",names(all.models2[x]),".html")) )) 
```

#### Anova
```{r nb-anova2}
# pull out anova tables
(anova2 <- do.call(rbind, lapply(all.models2, '[[', 'deviance')) %>% 
   mutate(across(is.numeric, round, 2)))
```

```{r save-model-deviance-tables2, include=FALSE}
 anova2 %>%
  kbl(escape = FALSE, 
      booktabs = TRUE, 
      format = "html",
      caption = "Type II Analysis of Deviance") %>%
  kable_styling() %>%
  cat(., file = file.path(tables_path,paste0(exp, "_deviance_tables_anova_conditions.html")))

```

#### Marginal mean contrasts
```{r t-wrangle-post-hoc-tables2, echo=FALSE}
contrasts2 <- do.call(rbind, lapply(all.models2, '[[', 'contrasts')) %>%
   mutate(across(is.numeric, round, 2)) %>%
   mutate(modulation = str_split(row.names(.),"[.]",simplify = T)[,1], .before=1) %>%
   remove_rownames() %>%
   dplyr::select(-c("df", "null")) %>% 
   dplyr::rename(IRR = ratio)

contrasts2
```

```{r t-save-post-hoc-tables2, include=FALSE}
# marginal means
contrasts2 %>%
    kbl(escape = FALSE, 
      booktabs = TRUE, 
      format = "html",
      caption = "Estimated Marginal Means") %>%
  kable_styling() %>%
  #kable_paper() %>%
  cat(., file = file.path(tables_path,paste0(exp, "_mm_contrasts_conditions.html"))) 
```

#### Incidence Rate Ratios (IRR)

```{r combined-IRRs2}
# IRR
IRRs2 <- do.call(rbind, lapply(all.models2, '[[', 'IRR')) %>%
   mutate(across(is.numeric, round, 2)) %>%
   mutate(condition = str_split(row.names(.),"[.]",simplify = T)[,1], .before=1) %>%
   mutate(coef = str_remove(str_split(row.names(.),"[.]",simplify = T)[,2],"condition"), .after=1) %>%
   remove_rownames()
IRRs2
```

```{r t-IRRs2, include=FALSE}
IRRkbl2 <- IRRs2 %>%
    kbl(escape = FALSE, 
      booktabs = TRUE, 
      format = "html",
      caption = "IRR") %>%
  kable_styling() 

IRRkbl2 %>%
  cat(., file = file.path(results_path,paste0("/tables/",exp,"_IRRs_conditions.html"))) 

```

```{r p-model-estimates2, include=FALSE}
lapply(seq_along(all.models2), function(x) {
       p <- all.models2[[x]]$plot 
       ggsave(file.path(sm_path,paste0(exp, '_IRR_', names(all.models2[x]),'.png') ), p, width = 5, height = 3)}) 

```



### Anticipating the trough or the peak?

Use bootstrapped means from the whole data set, we now want to see whether average
call onsets were "aimed" more at dodging AM peaks or more targeted at AM troughs.

To find out, we first compute two measures of call onset timing as follows:
```{r time-lag, include=FALSE}
# !! NB !! chunk depends on chunk `calculate-boot-parameters` - needs `mu_bs_df`

# bootstrapped means
mu_bs_time <- mu_bs_df %>%
  dplyr::filter(condition !="silence", !condition=="half-band masker") %>%
  mutate(f = as.numeric(str_remove(modulation,"Hz")),
         theta_t = round((mu/(2*pi)) * round(1/f,4),4),
         t_from_peak = theta_t - round((1/f)/2,4), # is 0 if on it, is > 0 if after, is < 0 if before
         t_to_trough = round((1/f),3) - theta_t, # relative to trough-to-trough phase, time to next trough
         t_from_trough = theta_t
         )

#mu_bs_time

# timings by condition
basic_timings <- mu_bs_time %>% group_by(condition) %>% 
  summarise(median_peak = median(t_from_peak)*1000, 
           iqr_peak = IQR(t_from_peak)*1000,
           #mad_peak = mad(t_from_peak),
           median_trough = median(t_to_trough)*1000,
           #mad_trough = mad(t_to_trough),
           iqr_trough = IQR(t_to_trough)*1000) %>%
  mutate(across(where(is.numeric),round,1))
basic_timings
```

Plot timings relative to both acoustic features. Show raw data (dots), distributions, 
and boxplots.

```{r plot-time-lag, out.width=50%}
comp.plot <- p.composite_peak_trough(mu_bs_time)

print(comp.plot)

curve_plots <- p.curves(mu_bs_time)

ggsave(file.path(figures_path,paste0(exp, '_fig4a.png')), comp.plot, width = w2+2, height = h-2)
ggsave(file.path(figures_path,paste0(exp, '_fig4a_curve.png')), curve_plots[[1]], width = (w2+2/2), height = h-2)

  
```


```{r timing-breakdown}
# timings by condition and modulation
more_timings <- mu_bs_time %>% group_by(modulation, condition) %>% 
  summarise(median_peak = median(t_from_peak)*1000, 
           iqr_peak = IQR(t_from_peak)*1000,
           #mad_peak = mad(t_from_peak),
           median_trough = median(t_to_trough)*1000,
           #mad_trough = mad(t_to_trough),
           iqr_trough = IQR(t_to_trough)*1000) %>%
  mutate(across(where(is.numeric),round,1))


more_timings

```


```{r p-time-lag, out.width="50%"}
pb <- p.peak_trough(mu_bs_time %>% 
                      dplyr::filter(!condition=="silence", !condition=="half-band masker"), binwidth = 8) %>%
  recolor()

print(pb)

ggsave(file.path(figures_path,paste0(exp, '_fig4a-alt1.png')), pb, width = 6, height = 4)

```

### Linear Classifiers
```{r lda-start}
mu_ld <- mu_bs_time %>% 
  dplyr::select(t_to_trough, t_from_peak, modulation) 
mu_ld <- mu_ld %>%
  mutate(modulation = make.names(mu_ld$modulation) %>% as_factor())

# split into 0.6:0.4 train/validation set
set.seed(seed)
#train_idx <- sample(1:nrow(mu_ld), size = floor(nrow(mu_ld)*0.60)) 
train_idx <- createDataPartition(mu_ld$modulation, p = .6, list = FALSE)

mu_bs_train <- mu_ld[train_idx,] 
mu_bs_valid <- mu_ld[-train_idx,] 


# cross-val 
ctrl <- trainControl(method = "repeatedcv", 10, repeats = 10, 
                     summaryFunction = multiClassSummary, classProbs = TRUE, savePredictions = TRUE)
```


First train some LDA models on troughs only, peaks only, or both:
```{r lda-train}
#--- Trough
trough_lda <- train(mu_bs_train[,1,drop=FALSE], mu_bs_train$modulation, 
                    method = "lda", trControl = ctrl, preProcess = c("center", "scale"))

##Generate LDA model predictions for bio indicator test set
trough_lda_pred <- predict(trough_lda, mu_bs_valid[,-3])

##Generate confusion matrix to show results
trough_lda_mat <- confusionMatrix(trough_lda_pred, mu_bs_valid$modulation)#, positive = "No")
trough_lda_mat

#--- Peak
peak_lda <- train(mu_bs_train[,2,drop=FALSE], mu_bs_train$modulation, 
                  method = "lda", trControl = ctrl, preProcess = c("center", "scale"))

##Generate LDA model predictions for bio indicator test set
peak_lda_pred <- predict(peak_lda, mu_bs_valid[,-3])

##Generate confusion matrix to show results
peak_lda_mat <- confusionMatrix(peak_lda_pred, mu_bs_valid$modulation) #, positive = "No")
peak_lda_mat


#--- Peak
full_lda <- train(mu_bs_train[,-3], mu_bs_train$modulation, 
                  method = "lda", trControl = ctrl, preProcess = c("center", "scale"))

##Generate LDA model predictions for bio indicator test set
full_lda_pred <- predict(full_lda, mu_bs_valid[,-3])

##Generate confusion matrix to show results
full_lda_mat <- confusionMatrix(full_lda_pred, mu_bs_valid$modulation) #, positive = "No")
full_lda_mat

lda_validation <- list(full_lda_mat, trough_lda_mat, peak_lda_mat)
```


Compare models
```{r lda-resamp}
results <- resamples(list(full=full_lda, trough=trough_lda, peak=peak_lda))

# ggplot(results) + 
#    labs(y = "Accuracy") + 
#    theme_linedraw()
# trellis.par.set(caretTheme())
# dotplot(results, metric = "Sensitivity")

summary(results)
```

Stats comparisons
```{r lda-resamp2}
# compare stats
difValues <- diff(results)
summary(difValues)
```


```{r lda-auc}
res <- evalm(list(full_lda, trough_lda, peak_lda),gnames=c('full','trough','peak'), rlinethick = 0.8)

pr <- 
  ggplot(res$prg$data) +
  geom_line(aes(x = FPR*100, y = SENS*100, color = Group), size = 2) +
  scale_color_brewer(name = NULL, expand = c(0,0)) +
  labs(x = "FPR [%]", y = "TPR [%]") +
  theme_jamlight() +
  theme_classic() +
  theme(panel.border = element_blank(),
      panel.spacing.y = unit(0.3, "cm"))
pr

  ggsave(file.path(figures_path,paste0(exp, '_fig4e.png')), 
         pr, width = 5, height = 4)
  ggsave(file.path(figures_path,paste0(exp, '_fig4e.pdf')), 
         pr, width = 5, height = 4)
```
```{r lda-plots}

confmats <- lapply(lda_validation, function(x) 
  cormat_to_df(x$table %>% as.data.frame() %>%  
                  mutate(Prediction = as_factor(str_remove(Prediction,"X")), 
                         Reference = as_factor(str_remove(Reference, "X")))) ) %>%
  setNames(c("full", "troughs", "peaks")) %>%
  map_df(., ~as.data.frame(.x), .id="model") %>%
  mutate(model = factor(model, levels = c("troughs", "peaks", "full")))


confmat <- p.mat(confmats, ulim = max((mu_bs_valid%>%group_by(modulation)%>%summarise(n=n()))[,2])) +
  facet_grid(cols=vars(model)) 
  print(confmat)
  
  ggsave(file.path(figures_path,paste0(exp, '_fig4b.png')), 
         confmat, width = w2, height = h) 
  ggsave(file.path(figures_path,paste0(exp, '_fig4b.pdf')), 
         confmat, width = w2, height = h)
  
  
```




### Quantify overlapping calls

```{r call-overlap}
overlap_calls <- data[which(data$onset_interval<data$duration),]%>% 
  mutate(overlap = duration-onset_interval)

cat("Of a total of", nrow(data), "calls,", nrow(overlap_calls),
    "(", round(nrow(overlap_calls)/nrow(data)*100,2), "%) overlapped in time.")

n_overlap <- overlap_calls %>% group_by(modulation, condition) %>% summarise(n = n())

(n_overlap_conttable <- addmargins(table(overlap_calls$modulation,overlap_calls$condition), c(1,2)))
```

#### Modelling call overlap incidence
Let's run the overlap data through a negative binomial...
```{r call-overlap-models, include=FALSE}
n_overlap <- overlap_calls %>% group_by(modulation, condition, group, as.factor(session)) %>% 
  summarise(n = n())

ovlp.models <- lapply(sort(unique(n_overlap$modulation)), function(x) {
                      print(x)
                      md <- run_negbinom_model(n_overlap %>% filter(modulation==x)) } )
```

Deviance tables...
```{r call-overlap-anovas}
(ovlp.anova <- do.call(rbind, lapply(ovlp.models, '[[', 'deviance')) %>% 
   mutate(across(is.numeric, round, 2)))
```

And contrasts...
```{r call-overlap-contrasts}
(contrasts <- do.call(rbind, lapply(ovlp.models, '[[', 'contrasts')) %>%
   mutate(across(is.numeric, round, 2)) %>%
   mutate(modulation = factor(rep(unique(n_overlap$modulation),each=length(unique(n_overlap$condition)))),.before=1) %>%
   remove_rownames() %>%
   dplyr::select(-c("df", "null")) %>%
   dplyr::rename(IRR = ratio))
```
```{r t-call-overlap-contrasts, include=FALSE}
# marginal means
contrasts %>%
    kbl(escape = FALSE, 
      booktabs = TRUE, 
      format = "html",
      caption = "Estimated Marginal Means") %>%
  kable_styling() %>%
  cat(., file = file.path(tables_path,paste0(exp, "_overlap_contrasts.html"))) 
```

And finally, predict values...
```{r call-overlap-predictions}
ovlp_preds <- do.call(rbind, lapply(ovlp.models, '[[', 'predictions')) %>%
   mutate(across(is.numeric, round, 2)) %>%
   mutate(modulation = factor(rep(unique(n_overlap$modulation),each=length(unique(n_overlap$condition))))) %>%
   remove_rownames()

ovlp_pred <- p.n_pred(ovlp_preds) %>% recolor()

ovlp_pred

ggsave(file.path(figures_path,paste0(exp, '_figS5b.png') ), ovlp_pred, width = 5, height = 4)

```

Finally, we can calculate circular stats for overlapping calls, and visualize the
angular vectors representing those distributions for each condition:
```{r sp-degree-overlap, echo=supplementary, out.width = "50%"}
overlap_circsummary <- overlap_calls %>%
    group_by(modulation, condition) %>%
    summarize(n = n(),
           theta_bar = mean.circular(start_phase_circ, na.rm = T),
           r_bar = rho.circular(start_phase_circ, na.rm = T),
           vm = var.circular(start_phase_circ), 
           rt = rayleigh.test(start_phase_circ, mu=circular(0))$statistic,
           p = rayleigh.test(start_phase_circ, mu=circular(0))$p.value,
           bs_mu_ci_l = mle.vonmises.bootstrap.ci(start_phase_circ, alpha=.05)$mu.ci[1], 
           bs_mu_ci_h = mle.vonmises.bootstrap.ci(start_phase_circ, alpha=.05)$mu.ci[2],) %>%
    mutate(p_adjust = p.adjust(p, method ="bonferroni"), .after = "p") %>%
    mutate(across(where(is.numeric), round, 3)) 

overlap_circsummary %>%
  kbl(escape = FALSE, 
      booktabs = TRUE, 
      format = "html",
      caption = "Circular statistics for overlapping calls") %>%
  kable_styling() %>%
  cat(., file = file.path(tables_path,paste0(exp, "_circ_stats_overlap.html")))

p.ang <- p.angular_vectors(overlap_circsummary %>% confint_wrap())

p.ang
 ggsave(file.path(figures_path,paste0(exp, '_figsS5a.png') ), p.ang, width = w, height = 4)
 
```

```{r sp-hist-overlap, eval=FALSE, include=FALSE, out.width="50%"}
# overlap histograms
p.ovlp <- ggplot(overlap_calls) +
  geom_rect(data = overlap_calls %>% group_by(modulation, condition) %>%
            summarise(quant75 = quantile(overlap,probs = seq(0,1,0.25))[4]),
            inherit.aes = F,
            aes(xmin = 0, xmax = quant75, ymin=0, ymax=Inf, fill = condition), alpha = 0.1)+
  geom_histogram(aes(overlap, color = condition, fill=condition), 
                 position = "identity", alpha=0.6) +
  scale_x_continuous(name = "overlap (ms)", 
                     limits = c(0,0.01), labels = 1000*seq(0,0.1,length.out=5)) +
  scale_color_manual(values=c_pal, name=NULL) +
  scale_fill_manual(values=c_pal, name=NULL) +
  facet_grid(cols=vars(modulation), scales="free") +
  guides(color=FALSE, fill=FALSE,alpha=FALSE) +
  theme_jamlight()
p.ovlp

 ggsave(file.path(figures_path,paste0(exp, '_figS5a-alt1.png') ), p.ovlp, width = w+2, height =4)
```

```{r p-call-overlap,  echo=supplementary, out.width = "50%"}
po <- p.circ_density(dat=overlap_calls, circ_dat=overlap_circsummary %>% confint_wrap(), bins = 30) %>% recolor(which = "f")
  print(po)
  ggsave(file.path(figures_path,paste0(exp, '_figS5a-alt2.png') ), po, width = w, height = 5)
  

```

### References
Some references for circular stats can be found in the following books: 

- [*Statistical analysis of circular data*](https://books.google.de/books?id=wGPj3EoFdJwC&printsec=frontcover#v=onepage&q&f=false)

- Batschelet, E. (1981). *Circular Statistics in Biology.* Academic Press: London.

- Jammalamadaka, S.R. & Sengupta, A. (2001). *Topics in Circular Statistics.* World Scientific, River Edge, N.J.
